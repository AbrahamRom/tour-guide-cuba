import streamlit as st
import json
from .src.recommender.src.user_profile import UserProfile
from .src.recommender.src.offer_loader import load_offers_from_directory
from .src.recommender.src.recommender import Recommender
from .src.recommender.src.utils import display_offer
from .src.rag.app.ollama_interface import OllamaClient


def render(state):
    # Session state
    if "selected_recommendations" not in state:
        state["selected_recommendations"] = []

    st.markdown(
        """
        <style>
        .main-title {font-size:2.5em; font-weight:bold; color:#2E8B57;}
        .subtitle {font-size:1.2em; color:#555;}
        .recommendation-card {background:#f8f9fa; border-radius:10px; padding:1em; margin-bottom:1em;}
        .stButton>button {background-color:#2E8B57; color:white;}
        </style>
        """,
        unsafe_allow_html=True,
    )

    st.markdown(
        '<div class="main-title">üå¥ SmartTour Cuba - Recommender</div>',
        unsafe_allow_html=True,
    )
    st.markdown(
        '<div class="subtitle">Descubre las mejores experiencias personalizadas en Cuba</div>',
        unsafe_allow_html=True,
    )

    # Show current user profile in sidebar
    st.sidebar.header("üë§ Perfil de Usuario Actual")
    if "collected_data" in state and state["collected_data"]:
        st.sidebar.success("‚úÖ Perfil cargado correctamente")
        with st.sidebar.expander("Ver datos del perfil"):
            st.sidebar.json(state["collected_data"])
    else:
        st.sidebar.warning("‚ö†Ô∏è No hay perfil de usuario disponible")
        st.sidebar.info("Complete la informaci√≥n en otras secciones de la aplicaci√≥n.")

    # LLM model and language selection in sidebar
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ü§ñ Configuraci√≥n del LLM")
    client = OllamaClient()

    # Verificar si Ollama est√° disponible
    if not client.is_ollama_available():
        st.sidebar.error("üî¥ Ollama no est√° disponible")
        st.sidebar.info("Para usar el sistema de recomendaciones con LLM, por favor:")
        st.sidebar.markdown("1. Instale Ollama desde https://ollama.ai")
        st.sidebar.markdown("2. Ejecute `ollama serve` en su terminal")
        st.sidebar.markdown("3. Descargue un modelo con `ollama pull llama2`")
        model_names = []
    else:
        st.sidebar.success("üü¢ Ollama conectado correctamente")
        models_response = client.list_models()
        if isinstance(models_response, dict) and "models" in models_response:
            models = models_response["models"]
        else:
            models = models_response
        model_names = [
            model["name"] if isinstance(model, dict) and "name" in model else str(model)
            for model in models
        ]

    # Solo mostrar selectores si hay modelos disponibles
    if model_names:
        selected_model = st.sidebar.selectbox(
            "Elige el modelo LLM",
            model_names,
            index=(
                0
                if "gemma:latest" not in model_names
                else model_names.index("gemma:latest")
            ),
            help="Selecciona el modelo de lenguaje para la explicaci√≥n.",
        )
        language = st.sidebar.selectbox(
            "Idioma de la explicaci√≥n",
            ["Espa√±ol", "English"],
            index=0,
            help="Selecciona el idioma en el que deseas la explicaci√≥n.",
        )
    else:
        selected_model = None
        language = "Espa√±ol"
        st.sidebar.warning("‚ö†Ô∏è Sin modelos LLM disponibles")
        st.sidebar.info("El sistema funcionar√° sin explicaciones de IA.")

    prompt_placeholder = {
        "Espa√±ol": (
            "De las siguientes ofertas, indica cu√°les son buenas o ideales para el usuario y por qu√©, "
            "y tambi√©n cu√°les no son convenientes y por qu√©. S√© espec√≠fico en tu explicaci√≥n."
        ),
        "English": (
            "From the following offers, indicate which ones are good or ideal for the user and why, "
            "and also which ones are not suitable and why. Be specific in your explanation."
        ),
    }

    # Check if user profile data exists in state
    if "collected_data" in state and state["collected_data"]:
        try:
            profile_json = state["collected_data"]
            user_profile = UserProfile(profile_json)
            offers = load_offers_from_directory("../DATA")
            recommender = Recommender(user_profile, offers)
            top_offers = recommender.rank_offers()
            if not top_offers:
                st.warning("No matching offers found or offer data is invalid.")
            else:
                st.subheader("üéØ Recomendaciones Principales")
                offer_keys = []
                offer_objs = []
                for idx, (score, offer) in enumerate(top_offers):
                    offer_title = offer.raw.get("name") or offer.raw.get("title")
                    offer_key = f"offer_{idx}_{offer_title}"
                    offer_keys.append(offer_key)
                    offer_objs.append(offer)
                    with st.expander(f"{offer_title} (Puntaje: {score:.2f})"):
                        # Muestra los campos de la oferta de forma estilizada sin particularizar nombres
                        details = ""
                        for k, v in offer.raw.items():
                            details += f"<b>{k.capitalize()}:</b> {v}<br>"
                        st.markdown(
                            f'<div class="recommendation-card">{details}</div>',
                            unsafe_allow_html=True,
                        )

                # LLM automatic explanation with spinner
                st.markdown("### ü§ñ ¬øQu√© es lo mejor para ti?")

                if selected_model and client.is_ollama_available():
                    prompt = (
                        f"{prompt_placeholder[language]}\n\nPerfil de usuario:\n{profile_json}\n\nOfertas:\n"
                        + "\n".join([str(o.raw) for o in offer_objs])
                    )
                    explanation = ""
                    explanation_placeholder = st.empty()
                    with st.spinner("El modelo LLM est√° generando la explicaci√≥n..."):
                        # Intenta imprimir cada chunk directamente para debug
                        for chunk in client.stream_generate(selected_model, prompt):
                            try:
                                # Si el chunk es texto plano, √∫salo directamente
                                if isinstance(chunk, str):
                                    try:
                                        # Intenta decodificar como JSON, si falla, es texto plano
                                        data = json.loads(chunk)
                                        text = data.get("response", "")
                                        # Solo procesar si hay texto y no est√° marcado como terminado
                                        if text and not data.get("done", False):
                                            explanation += text
                                            explanation_placeholder.markdown(
                                                explanation
                                            )
                                        # Si est√° marcado como terminado, salir del bucle
                                        elif data.get("done", False):
                                            break
                                    except Exception:
                                        # Si no es JSON v√°lido, es texto plano
                                        text = chunk
                                        explanation += text
                                        explanation_placeholder.markdown(explanation)
                                else:
                                    # Si es dict, busca el campo 'response'
                                    text = chunk.get("response", "")
                                    if text and not chunk.get("done", False):
                                        explanation += text
                                        explanation_placeholder.markdown(explanation)
                                    elif chunk.get("done", False):
                                        break
                            except Exception:
                                # Como √∫ltimo recurso, usar el chunk como string si no est√° vac√≠o
                                text = str(chunk).strip()
                                if text and not text.startswith('{"model"'):
                                    explanation += text
                                    explanation_placeholder.markdown(explanation)
                    # Si no se imprimi√≥ nada, muestra un mensaje de error
                    if not explanation.strip():
                        st.error(
                            "No se recibi√≥ explicaci√≥n del modelo. Verifica que el modelo est√© funcionando correctamente o revisa la conexi√≥n."
                        )
                else:
                    st.info("ü§ñ **Explicaci√≥n con IA no disponible**")
                    st.markdown(
                        """
                    Para obtener explicaciones personalizadas con IA, necesitas:
                    1. Instalar Ollama desde https://ollama.ai
                    2. Ejecutar `ollama serve` en tu terminal
                    3. Descargar un modelo con `ollama pull llama2`
                    
                    **Mientras tanto, puedes revisar manualmente las recomendaciones bas√°ndote en:**
                    - Tu presupuesto y preferencias de viaje
                    - Las calificaciones y caracter√≠sticas de cada oferta
                    - La ubicaci√≥n y facilidades disponibles
                    """
                    )

                # User selects offers with checkboxes (only after LLM finishes)
                st.markdown("### ‚úÖ Selecciona las ofertas que te interesan")
                selected = []
                for offer_key, offer in zip(offer_keys, offer_objs):
                    checked = st.checkbox(
                        offer.raw.get("name") or offer.raw.get("title"), key=offer_key
                    )
                    if checked:
                        selected.append(offer.raw)
                state["selected_recommendations"] = selected

        except Exception as e:
            st.error(f"Error procesando el perfil del usuario: {e}")
    else:
        st.info(
            "No se ha recopilado informaci√≥n del usuario. Por favor, completa tu perfil en las otras secciones de la aplicaci√≥n para obtener recomendaciones personalizadas."
        )
